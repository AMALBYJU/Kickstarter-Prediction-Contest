---
title: "Predicting Successful Kickstarter Project"
author: "Group-26"
output:
  html_document: 
    toc: yes
---
\vspace{0.25in}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
library(gbm)
library(glmnet)
library(xgboost)
library(e1071)
library(tidyverse)
library(tm)
library(text2vec)
library(SnowballC)
library(vip)
library(caret)
library(class)
library(mice)
library(ROCR)
```

## Loading the data 

The data-sets are loaded and external data-set containing both the user-country and user-state was added to a larger data-set.

```{r}
knitr::opts_chunk$set(echo = TRUE)
#load data files
train_x <- read_csv("ks_training_X.csv") %>%
  mutate(original_TR = 1)
train_y <- read_csv("ks_training_y.csv")
test_x <- read_csv("ks_test_X.csv") %>%
  mutate(original_TR = 0)

#rbind binds rows
total <- rbind(train_x, test_x)

#join the training y to the training x file
#also turn two of the target variables into factors
total <- total %>%
  left_join(train_y, by = "id")

#loading external data-set to extract creator country and state
load("38050-0003-Data.rda")

da38050.0003 <- da38050.0003 %>%
  distinct(PID, .keep_all = TRUE)

#combining to create a new dataset 
total <- total %>%
  left_join(da38050.0003, by = c(id = "PID"))
```

```{r}
summary(total)
```


## Cleaning and feature engineering 

Cleaned the reward amounts to get max,min and num_rewards

```{r}
knitr::opts_chunk$set(echo = TRUE)
# cleaning the reward amounts 
find_length <- function(ch){
  ifelse(is.na(ch) | (sum(is.na(as.numeric(strsplit(ch, ",")[[1]]))) > 0), 0, length(as.numeric(strsplit(ch, ",")[[1]])))
}

find_min <- function(ch){
  ifelse(is.na(ch) | (sum(is.na(as.numeric(strsplit(ch, ",")[[1]]))) > 0), NA, min(as.numeric(strsplit(ch, ",")[[1]])))
}

find_max <- function(ch){
  ifelse(is.na(ch) | (sum(is.na(as.numeric(strsplit(ch, ",")[[1]]))) > 0), NA, max(as.numeric(strsplit(ch, ",")[[1]])))
}

find_sd <- function(ch){
  ifelse(is.na(ch) | (sum(is.na(as.numeric(strsplit(ch, ",")[[1]]))) > 0), NA, sd(as.numeric(strsplit(ch, ",")[[1]])))
}

total$num_rewards <- as.numeric(lapply(total$reward_amounts, find_length))
total$min_reward <- as.numeric(lapply(total$reward_amounts, find_min))
total$max_reward <- as.numeric(lapply(total$reward_amounts, find_max))
total$sd_reward <- as.numeric(lapply(total$reward_amounts, find_sd))
```

Selecting all the columns to a new data set

```{r}
total_success <- total %>%
  select(success, goal, id, name, blurb, captions, tag_names, USER_LOCATION_COUNTRY, USER_LOCATION_STATE,
         region, category_parent, num_words, category_name, avg_wordlengths, deadline, launched_at, created_at,
         grade_level, minage_creator, location_slug, smiling_creator, sentence_counter, minage_project,
         ADV, NOUN, ADP, PRT, DET, PRON, VERB, NUM, CONJ, ADJ, male_creator, female_creator, numfaces_project,
         afinn_pos, afinn_neg, original_TR, contains_youtube, numfaces_creator, smiling_project,
         num_rewards, min_reward, max_reward, sd_reward, maxage_creator, avgsentencelength, reward_descriptions)
```

Changing the region and category parent to factors to understand and create features

```{r}
total_success <- total_success %>%
  mutate(region = as.factor(region),
       category_parent = as.factor(category_parent))

plot(total_success$region)
```
```{r}
plot(total_success$category_parent)
```

Converting time-series and creating time_gap = deadline - launch_date

```{r}
total_success <- total_success %>%
  mutate(deadline = as.Date(deadline),
         created_at = as.Date(created_at),
         launched_at = as.Date(launched_at))
```

Converting the location_slug to factors 

```{r}
total_success <- total_success %>%
  group_by(location_slug) %>%
  mutate(location_slug_freq = n()) %>%
  ungroup() %>%
  mutate(location_slug = ifelse(location_slug_freq < 1500, 'Other Location Slug', location_slug),
         location_slug = ifelse(is.na(location_slug), 'Other Location Slug', location_slug),
         location_slug = as.factor(location_slug))

summary(total_success$location_slug)
```

Using the user_location_country from the external data set , with US having nearly 80K projects

**Note: We found that this is an important factor since the actual location of the project doesn't correspond to the location of the user**

```{r}
total_success <- total_success %>%
  group_by(USER_LOCATION_COUNTRY) %>%
  mutate(location_country_freq = n()) %>%
  ungroup() %>%
  mutate(USER_LOCATION_COUNTRY = as.character(USER_LOCATION_COUNTRY),
         USER_LOCATION_COUNTRY = ifelse(location_country_freq < 10 | USER_LOCATION_COUNTRY == "  " | is.na(USER_LOCATION_COUNTRY), 'Other Country', USER_LOCATION_COUNTRY),
         USER_LOCATION_COUNTRY = as.factor(USER_LOCATION_COUNTRY))

summary(total_success$USER_LOCATION_COUNTRY)
```

Using the user_location_country from the external data set , with California having nearly 24K projects

```{r}
total_success <- total_success %>%
  group_by(USER_LOCATION_STATE) %>%
  mutate(location_state_freq = n()) %>%
  ungroup() %>%
  mutate(USER_LOCATION_STATE = as.character(USER_LOCATION_STATE),
         USER_LOCATION_STATE = ifelse(location_state_freq < 100 | USER_LOCATION_STATE == "   " | is.na(USER_LOCATION_STATE), 'Other State', USER_LOCATION_STATE),
         USER_LOCATION_STATE = as.factor(USER_LOCATION_STATE))

summary(total_success$USER_LOCATION_STATE)
```
Cleaning the values for category_name and reducing the number of factors

```{r}
total_success <- total_success %>%
  group_by(category_name) %>%
  mutate(category_name_freq = n()) %>%
  ungroup() %>%
  mutate(category_name = ifelse(category_name_freq < 3000, 'Other Category', category_name),
         category_name = ifelse(is.na(category_name), 'Other Category', category_name),
         category_name = as.factor(category_name))

summary(total_success$category_name)
```

Creating new features like afinn_overall = afinn_pos - afinn_neg : gives overall sentiment of the model ,
changing success to binary 1 & 0 for easy modeling, proj_duration = launched_at - created_at: gives the estimated duration of the project,launched_at = changed to numeric which gives the seconds as compared to 1970 , extra_female_creators= female_creator - male_creator : to find the if the creators are male dominant or female dominant. Changing if the project has YouTube to a factor ,
introducing to unstructured text so that values can removed after text mining. 

```{r}
total_success <- total_success %>%
  mutate(afinn_overall = afinn_pos - afinn_neg,
         afinn_overall = ifelse(is.na(afinn_overall), 0, afinn_overall))
```

```{r}
total_success <- total_success %>%
  mutate(success = ifelse(success == "YES", 1, 0))
```

```{r}
total_success <- total_success %>%
  mutate(proj_duration = as.numeric(launched_at - created_at))
```

Converted the project launch date to day of the week and found that most projects were launched on a **Tuesday**

```{r}
total_success <- total_success %>%
  mutate(day_of_week = as.factor(as.character(format(launched_at, format="%A"))))

summary(total_success$day_of_week)
check <- table(total_success$success, total_success$day_of_week)
prop.table(check, 2)
barplot(check, legend.text = rownames(check))
```

```{r}
total_success <- total_success %>%
  mutate(launched_at = as.numeric(launched_at))
```

```{r}
total_success <- total_success %>%
  mutate(reward_descriptions = ifelse(is.na(reward_descriptions), "None", reward_descriptions),
         blurb = ifelse(is.na(blurb), "None", blurb),
         name = ifelse(is.na(name), "None", name), 
         captions = ifelse(is.na(captions), "None", captions),
         tag_names = ifelse(is.na(tag_names), "None", tag_names))
```

```{r}
total_success <- total_success %>%
  mutate(extra_female_creators = female_creator - male_creator)
```

```{r}
total_success <- total_success %>%
  mutate(contains_youtube = as.factor(contains_youtube))

summary(total_success$contains_youtube)
check <- table(total_success$success, total_success$contains_youtube)
prop.table(check, 2)
barplot(check, legend.text = rownames(check))

```

```{r}
total_success <- total_success %>%
  mutate(time_gap = as.numeric(deadline - launched_at))
```

Understanding Goal, we see the variable goal has values centered around 0 -5000 and huge outliers and thus without categorizing we'd like to see the impact of goal on the success variable and interact with few other terms like category etc. 

```{r}
hist(total_success$goal)
```

```{r}
summary(total_success)
```

## Formula for model building 

```{r}
formula <- success ~ USER_LOCATION_STATE+USER_LOCATION_COUNTRY+goal+region+category_parent+num_words+grade_level+minage_creator+avg_wordlengths+time_gap+location_slug+category_name+afinn_overall+contains_youtube+avgsentencelength+numfaces_creator+sentence_counter+ADV+NOUN+ADP+PRT+DET+VERB+CONJ+num_rewards+min_reward+sd_reward+max_reward+extra_female_creators+numfaces_project+minage_project+smiling_creator+proj_duration:goal+category_parent:goal+maxage_creator:goal+proj_duration:numfaces_creator+num_words:proj_duration+contains_youtube:goal+goal:max_reward+grade_level:maxage_creator+afinn_overall:category_parent+grade_level:category_parent+day_of_week
```

## Adding Original to allow split for labeled and use_data

```{r}
labeled_data <- total_success %>%
  filter(original_TR == 1)

use_data <- total_success %>%
  filter(original_TR == 0) 

original_TR_indices <- which(as.logical(total_success$original_TR))
```

## Text featuriztion of Blurb , Captions , Name , Reward desc ,Tag_names

Unstructured text was pruned with the not useful stopwords, stemming was done , punctuations were removed, numbers were removed .
Having uni- and bi-  grams and term_count_min = 20, doc_count_min = 10. 

```{r}
# Reward Description Column

cleaning_tokenizer <- function(v) {
  v %>%
    removeNumbers %>% #remove all numbers
    removePunctuation %>% #remove all punctuation
    removeWords(stopwords(kind="en")) %>% #remove stopwords
    stemDocument %>%
    word_tokenizer 
}

prep_fun = tolower
tok_fun = cleaning_tokenizer



it_reward = itoken(total_success$reward_descriptions,
                   preprocessor = prep_fun, 
                   tokenizer = tok_fun, 
                   ids = total_success$id, 
                   progressbar = FALSE)

stop_words_reward = c("None", "will", "copi", "plus", "receiv", "get", "one", "includ", "ship", "special", "also", "print", "will_receiv", "download", "edit", "x", "choic", "list", "us", "well", "can", "add", "two", "limit", "youll", "limit_edit")
vocab_reward = create_vocabulary(it_reward, ngram = c(1L, 2L), stopwords = stop_words_reward)

vocab_reward <- prune_vocabulary(vocab_reward, term_count_min = 20, doc_count_min = 10)

vectorizer_reward = vocab_vectorizer(vocab_reward)

dtm_reward = create_dtm(it_reward, vectorizer_reward)

# Name column

it_name = itoken(total_success$name,
                 preprocessor = prep_fun, 
                 tokenizer = tok_fun, 
                 ids = total_success$id, 
                 progressbar = FALSE)

stop_words_name = c("None", "new", "short", "help", "make", "one", "live", "collect", "get")

vocab_name = create_vocabulary(it_name, ngram = c(1L, 2L), stopwords = stop_words_name)

vocab_name <- prune_vocabulary(vocab_name, term_count_min = 20, doc_count_min = 10)

vectorizer_name = vocab_vectorizer(vocab_name)

dtm_name = create_dtm(it_name, vectorizer_name)

# Blurb column

it_blurb = itoken(total_success$blurb,
                  preprocessor = prep_fun, 
                  tokenizer = tok_fun, 
                  ids = total_success$id, 
                  progressbar = FALSE)
stop_words_blurb <- c("None", "help", "new", "one", "short", "two", "like", "start", "build", "come", "look", "go", "made", "will", "make", "need", "creat", "us", "get", "can", "live", "want", "bring", "work", "take", "use", "play")
vocab_blurb = create_vocabulary(it_blurb, ngram = c(1L, 2L), stopwords = stop_words_blurb)

vocab_blurb <- prune_vocabulary(vocab_blurb, term_count_min = 20, doc_count_min = 10)

vectorizer_blurb = vocab_vectorizer(vocab_blurb)

dtm_blurb = create_dtm(it_blurb, vectorizer_blurb)

# Captions column

it_captions = itoken(total_success$captions,
                     preprocessor = prep_fun, 
                     tokenizer = tok_fun, 
                     ids = total_success$id, 
                     progressbar = FALSE)

stop_words_captions <- c("none", "close", "front", "white", "black", "red", "blue", "larg", "green", "yellow", "next")
vocab_captions = create_vocabulary(it_captions, ngram = c(1L, 2L))

vocab_captions <- prune_vocabulary(vocab_captions, term_count_min = 20, doc_count_min = 10)

vectorizer_captions = vocab_vectorizer(vocab_captions)

dtm_captions = create_dtm(it_captions, vectorizer_captions)

# Tag names column

it_tag_names = itoken(total_success$tag_names,
                      preprocessor = prep_fun, 
                      tokenizer = tok_fun, 
                      ids = total_success$id, 
                      progressbar = FALSE)

stop_words_tag_names <- c("none", "white", "fast", "black")
vocab_tag_names = create_vocabulary(it_tag_names, ngram = c(1L, 2L))

vocab_tag_names <- prune_vocabulary(vocab_tag_names, term_count_min = 20, doc_count_min = 10)

vectorizer_tag_names = vocab_vectorizer(vocab_tag_names)

dtm_tag_names = create_dtm(it_tag_names, vectorizer_tag_names)
```

## Converting the data to matrix for labeled and use_data


```{r}
labeled_data_matrix <- model.matrix(formula, model.frame(~., labeled_data, na.action=na.pass)) 

labeled_data_matrix <- cbind(labeled_data_matrix, dtm_reward[original_TR_indices,], dtm_name[original_TR_indices,], dtm_blurb[original_TR_indices,], dtm_captions[original_TR_indices,])

use_data_matrix <- model.matrix(formula,model.frame(~ ., use_data, na.action=na.pass))

use_data_matrix <- cbind(use_data_matrix, dtm_reward[-original_TR_indices,], dtm_name[-original_TR_indices,], dtm_blurb[-original_TR_indices,], dtm_captions[-original_TR_indices,]) 

```

## Splitting and validation selection 

Simple split 70-30 was preferred since the choice of modelling was going to ensemble which would include bagging and boosting and cross-validation would be implemented in other models. 

```{r}
set.seed(1)
training_indices <- sample(nrow(labeled_data_matrix), .7*nrow(labeled_data_matrix))
tr_x <- labeled_data_matrix[training_indices,]
va_x <- labeled_data_matrix[-training_indices,]

tr_y <- labeled_data[training_indices,]$success
va_y <- labeled_data[-training_indices,]$success
```

## Best Model : XGBoost

XGBoost with the hyper parameters max.depth = 4, eta = 0.2, nrounds = 900 gave the highest accuracy and hence was selected .

```{r}
knitr::opts_chunk$set(echo = TRUE)
bst <- xgboost(data = tr_x, label = tr_y, max.depth = 4, eta = 0.2, nrounds = 900,  objective = "binary:logistic")
```

```{r}
# Variable importance chart 
importance_matrix = xgb.importance(colnames(labeled_data_matrix), model = bst)
imp.df <- data.frame(importance_matrix)
xgb.plot.importance(importance_matrix[1:20,])
```


```{r}
bst_pred <- predict(bst, va_x, type='response')
bst_classifications <- ifelse(bst_pred > 0.5, 1, 0)
bst_acc <- mean(ifelse(bst_classifications == va_y, 1, 0))

bst_acc

# Predict for use data
bst_pred_use <- predict(bst, use_data_matrix, type='response')
preds_class <- ifelse(bst_pred_use>.5,"YES","NO")

#output your predictions
write.table(preds_class, "success_group26.csv", row.names = FALSE)
```

## Other Models : Boosting 

```{r}
set.seed(1)
training_indices_boost <- sample(nrow(labeled_data), .7*nrow(labeled_data))
tr <- labeled_data[training_indices_boost,]
va <- labeled_data[-training_indices_boost,]

tr_boost <- labeled_data[training_indices_boost,]$success
va_boost <- labeled_data[-training_indices_boost,]$success

boost.mod <- gbm(formula,data=tr,
                 distribution="bernoulli",
                 n.trees=5000,
                 interaction.depth=3)
```
```{r}
boost_preds <- predict(boost.mod,newdata=va,type='response',n.trees=5000)

boost_class <- ifelse(boost_preds>.5,1,0)
boost_acc <- mean(ifelse(boost_class==va_boost,1,0))
boost_acc
```

## Ridge and Lasso with 5-fold cross validation

```{r}
# grid for ridge and lasso
grid <- 10^seq(20,-20,length=100)

#glmnet automatically does cross-validation for you, you just need to specify k. 
k<-5

model <- success ~ USER_LOCATION_COUNTRY+goal+region+category_parent+num_words+grade_level+minage_creator+avg_wordlengths+time_gap+location_slug+category_name+afinn_overall+contains_youtube+avgsentencelength+numfaces_creator+sentence_counter+ADV+NOUN+ADP+PRT+DET+VERB+CONJ+extra_female_creators+numfaces_project+minage_project+smiling_creator+proj_duration:goal+category_parent:goal+maxage_creator:goal+proj_duration:numfaces_creator+num_words:proj_duration+contains_youtube:goal+grade_level:maxage_creator+afinn_overall:category_parent+grade_level:category_parent+day_of_week

labeled_data_matrix_log <- model.matrix(model, model.frame(~., labeled_data, na.action=na.pass)) 

use_data_matrix_log <- model.matrix(model,model.frame(~ ., use_data, na.action=na.pass))

set.seed(1)
training_indices <- sample(nrow(labeled_data_matrix_log), .7*nrow(labeled_data_matrix_log))
tr_x_log <- labeled_data_matrix_log[training_indices,]
va_x_log <- labeled_data_matrix_log[-training_indices,]

tr_y_log <- labeled_data[training_indices,]$success
va_y_log <- labeled_data[-training_indices,]$success
```


```{r}
#family="binomial" yields logistic regression; family="gaussian" yields linear regression
#alpha = 1 yields the lasso penalty, and alpha= 0 the ridge penalty
cv.out.lasso <- cv.glmnet(tr_x_log , tr_y_log, family="binomial", alpha=1, lambda=grid, nfolds=k)
plot(cv.out.lasso)
```


```{r}
bestlam.lasso <- cv.out.lasso$lambda.min
bestlam.lasso
```


```{r}
pred.lasso <- predict(cv.out.lasso, s=bestlam.lasso, newx = va_x_log)
valid_classifications.lasso <- ifelse(pred.lasso >= 0.5, 1, 0)
valid_acc.lasso <- mean(ifelse(valid_classifications.lasso == va_y, 1, 0))

valid_acc.lasso
```


```{r}
cv.out.ridge <- cv.glmnet(tr_x_log , tr_y_log, family="binomial", alpha=0, lambda=grid, nfolds=k)
plot(cv.out.ridge)
```


```{r}
bestlam.ridge <- cv.out.ridge$lambda.min
bestlam.ridge
```


```{r}
pred.ridge <- predict(cv.out.ridge, s=bestlam.ridge, newx = va_x_log)
valid_classifications.ridge <- ifelse(pred.ridge >= 0.5, 1, 0)
valid_acc.ridge <- mean(ifelse(valid_classifications.ridge == va_y, 1, 0))

valid_acc.ridge
```

## Logistic Regression

```{r}
# log
log_model <- glm(formula, data = tr, family = "binomial")
```


```{r}
prediction_log <- predict(log_model, newdata = va , type = "response")
prediction_log <- ifelse(is.na(prediction_log),0,prediction_log)
classification_log <- ifelse(prediction_log > .5, 1 , 0)   

#Evaluation, assessing if the predictions match the actual values or not, then calculate the accuracy
correct_log <- ifelse(classification_log == va_boost, 1, 0) 
accuracy_log = sum(correct_log)/length(correct_log)

accuracy_log
```

## KNN  - K=5

```{r}
knn.pred=knn(tr_x_log, #the training instances' features
             va_x_log, #the new instances we want to make predictions for
             tr_y_log, #the y values in the training data
             k=5, #choose k
             prob = TRUE) #get probabilities as well
```


```{r}
knn.probs <- attr(knn.pred, "prob")
knn.prob_of_positive <- ifelse(knn.pred == 1, knn.probs, 1-knn.probs)
knn_classifications<- ifelse(knn.prob_of_positive > 0.5, 1, 0)
knn_acc <- mean(ifelse(knn_classifications == va_boost, 1, 0))
knn_acc
```


## Fitting Curves K-NN

```{r}
kvec <- c(1,2,3,5) #you can change this if you want

#initialize storage
va_acc <- rep(0, length(kvec))
tr_acc <- rep(0, length(kvec))

accuracy <- function(classifications, actuals){
  correct_classifications <- ifelse(classifications == actuals, 1, 0)
  acc <- sum(correct_classifications)/length(classifications)
  return(acc)
}

#for loop
for(i in 1:length(kvec)){
  k <- kvec[i] #get the ith k in kvec
  
  #compute predictions using train.X as the potential neighbors
  #new points to be classified are first validation, then training points
  va_preds <- knn(tr_x_log, va_x_log, tr_y_log, k = k)
  tr_preds <- knn(tr_x_log, tr_x_log, tr_y_log, k = k)
  
  #compute the accuracy for each set of predictions
  va_accuracy <- accuracy(va_preds, va_y_log)
  tr_accuracy <- accuracy(tr_preds, tr_y_log)
  
  #store in the appropriate place
  va_acc[i] <- va_accuracy
  tr_acc[i] <- tr_accuracy
  
}

#make the plot
plot(kvec, va_acc, col = 'red', type = 'l', ylim = c(0.5,1))
lines(kvec, tr_acc, col = 'blue')
```

## Fitting curves Ridge & Lasso

```{r}
##Lasso
lambdas <- cv.out.lasso$lambda
errors <- cv.out.lasso$cvm

#with this particular set of lambdas, you can see the curve easier if you plot them on a log-log scale
plot(log(lambdas), errors)
```

```{r}
lambdas_ridge <- cv.out.ridge$lambda
errors_ridge <- cv.out.ridge$cvm


#with this particular set of lambdas, you can see the curve easier if you plot them on a log-log scale
plot(log(lambdas_ridge), errors_ridge)
```

## Fitting Curve XgBoost

```{r}
#three hyperparameters can possibly really change predictive performance of xgboost (although maybe not)
depth_choose <- c(1, 2 , 4 , 5 , 7 , 9 )

  
va_acc <- rep(0, length(depth_choose))
tr_acc <- rep(0, length(depth_choose))



for(j in c(1:length(depth_choose)))
  {
        thisnrounds <- depth_choose[j]
        
        inner_bst <- xgboost(data = tr_x,  label = tr_y, max.depth = depth_choose , eta = 0.2, nrounds = 100,  objective = "binary:logistic", verbosity = 0, verbose = 0)
        
        inner_bst_pred_valid <- predict(inner_bst, va_x, type= "response")
        inner_bst_classifications_valid <- ifelse(inner_bst_pred_valid > 0.5, 1, 0)
        inner_bst_acc_valid <- mean(ifelse(inner_bst_classifications_valid == va_y, 1, 0))
        
        inner_bst_pred_train <- predict(inner_bst, tr_x, type= "response")
        inner_bst_classifications_train <- ifelse(inner_bst_pred_train > 0.5, 1, 0)
        inner_bst_acc_train <- mean(ifelse(inner_bst_classifications_train == tr_y, 1, 0))
        
          va_acc[j] <- inner_bst_acc_valid
          tr_acc[j] <- inner_bst_acc_train
  }
  

#make the plot
plot(depth_choose, va_acc, col = 'red', type = 'l', ylim = c(0.3,1))
lines(depth_choose, tr_acc, col = 'blue')
```

```{r}
#three hyperparameters can possibly really change predictive performance of xgboost (although maybe not)
eta_choose <- c(0.2,0.3,0.5 )

  
va_acc <- rep(0, length(eta_choose))
tr_acc <- rep(0, length(eta_choose))



for(j in c(1:length(eta_choose)))
  {
        thisnrounds <- eta_choose[j]
        
        inner_bst <- xgboost(data = tr_x,  label = tr_y, max.depth = 4 , eta = eta_choose, nrounds = 100,  objective = "binary:logistic", verbosity = 0, verbose = 0)
        
        inner_bst_pred_valid <- predict(inner_bst, va_x, type= "response")
        inner_bst_classifications_valid <- ifelse(inner_bst_pred_valid > 0.5, 1, 0)
        inner_bst_acc_valid <- mean(ifelse(inner_bst_classifications_valid == va_y, 1, 0))
        
        inner_bst_pred_train <- predict(inner_bst, tr_x, type= "response")
        inner_bst_classifications_train <- ifelse(inner_bst_pred_train > 0.5, 1, 0)
        inner_bst_acc_train <- mean(ifelse(inner_bst_classifications_train == tr_y, 1, 0))
        
          va_acc[j] <- inner_bst_acc_valid
          tr_acc[j] <- inner_bst_acc_train
  }
  

#make the plot
plot(eta_choose, va_acc, col = 'red', type = 'l', ylim = c(0.3,1))
lines(eta_choose, tr_acc, col = 'blue')
```


```{r}
#three hyperparameters can possibly really change predictive performance of xgboost (although maybe not)
nrounds_choose <- c(100, 200 , 400 , 500 , 700 , 900 , 1000)

  
va_acc <- rep(0, length(nrounds_choose))
tr_acc <- rep(0, length(nrounds_choose))



for(j in c(1:length(nrounds_choose)))
  {
        thisnrounds <- nrounds_choose[j]
        
        inner_bst <- xgboost(data = tr_x,  label = tr_y, max.depth = 4, eta = 0.2, nrounds = thisnrounds,  objective = "binary:logistic", verbosity = 0, verbose = 0)
        
        inner_bst_pred_valid <- predict(inner_bst, va_x, type= "response")
        inner_bst_classifications_valid <- ifelse(inner_bst_pred_valid > 0.5, 1, 0)
        inner_bst_acc_valid <- mean(ifelse(inner_bst_classifications_valid == va_y, 1, 0))
        
        inner_bst_pred_train <- predict(inner_bst, tr_x, type= "response")
        inner_bst_classifications_train <- ifelse(inner_bst_pred_train > 0.5, 1, 0)
        inner_bst_acc_train <- mean(ifelse(inner_bst_classifications_train == tr_y, 1, 0))
        
          va_acc[j] <- inner_bst_acc_valid
          tr_acc[j] <- inner_bst_acc_train
  }
  

#make the plot
plot(nrounds_choose, va_acc, col = 'red', type = 'l', ylim = c(0.3,1))
lines(nrounds_choose, tr_acc, col = 'blue')
```

## Lift Curve , ROC  for all models & AUC for XgBoost

```{r}
bst_auc <- prediction(bst_pred, va_y)
performance(bst_auc, measure = "auc")@y.values[[1]]
```

```{r}
boost_auc <- prediction(boost_preds, va_boost)
ridge_auc <- prediction(pred.ridge, va_y_log)
lasso_auc <- prediction(pred.lasso, va_y_log)
log_auc <- prediction(prediction_log, va_boost)
knn_auc <- prediction(knn.prob_of_positive, va_y_log)

lift_bst <- performance(bst_auc, "lift", "rpp")
lift_lasso <- performance(lasso_auc, "lift", "rpp")
lift_ridge <- performance(ridge_auc, "lift", 'rpp')
lift_log <- performance(log_auc, "lift", 'rpp')
lift_boost <- performance(boost_auc, "lift", 'rpp')
lift_knn <- performance(knn_auc, "lift", 'rpp')

roc_bst <- performance(bst_auc, "tpr", "fpr")
roc_lasso <- performance(lasso_auc, "tpr", "fpr")
roc_ridge <- performance(ridge_auc, "tpr", "fpr")
roc_log <- performance(log_auc,"tpr", "fpr")
roc_boost <- performance(boost_auc, "tpr", "fpr")
roc_knn <- performance(knn_auc, "tpr", "fpr")


plot(lift_bst, col = "red", lwd = 2)
plot(lift_lasso, add = T, col  = "blue", lwd = 2)
plot(lift_boost, add = T, col = "forestgreen", lwd = 2)
plot(lift_ridge, add = T, col = "brown", lwd = 2)
plot(lift_log, add = T, col = "gray", lwd = 2)
plot(lift_knn, add = T, col = "black", lwd = 2)
```

```{r}
plot(roc_bst, col = "red", lwd = 2)
plot(roc_lasso, add = T, col  = "blue", lwd = 2)
plot(roc_ridge, add = T, col = "forestgreen", lwd = 2)
plot(roc_log, add = T, col = "brown", lwd = 2)
plot(roc_boost, add = T, col = "gray", lwd = 2)
plot(roc_knn, add = T, col = "black", lwd = 2)
```






